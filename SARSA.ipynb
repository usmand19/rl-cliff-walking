{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "%matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "\n",
    "def change_range(values, vmin=0, vmax=1):\n",
    "    start_zero = values - np.min(values)\n",
    "    return (start_zero / (np.max(start_zero) + 1e-7)) * (vmax - vmin) + vmin\n",
    "\n",
    "class GridWorld:\n",
    "    terrain_color = dict(normal=[127/360, 0, 96/100],\n",
    "                         objective=[26/360, 100/100, 100/100],\n",
    "                         cliff=[247/360, 92/100, 70/100],\n",
    "                         player=[344/360, 93/100, 100/100])\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.player = None\n",
    "        self._create_grid()  \n",
    "        self._draw_grid()\n",
    "        \n",
    "    def _create_grid(self, initial_grid=None):\n",
    "        self.grid = self.terrain_color['normal'] * np.ones((4, 12, 3))\n",
    "        self._add_objectives(self.grid)\n",
    "        \n",
    "    def _add_objectives(self, grid):\n",
    "        grid[-1, 1:11] = self.terrain_color['cliff']\n",
    "        grid[-1, -1] = self.terrain_color['objective']\n",
    "        \n",
    "    def _draw_grid(self):\n",
    "        self.fig, self.ax = plt.subplots(figsize=(12, 4))\n",
    "        self.ax.grid(which='minor')       \n",
    "        self.q_texts = [self.ax.text(*self._id_to_position(i)[::-1], '0',\n",
    "                                     fontsize=11, verticalalignment='center', \n",
    "                                     horizontalalignment='center') for i in range(12 * 4)]     \n",
    "         \n",
    "        self.im = self.ax.imshow(hsv_to_rgb(self.grid), cmap='terrain',\n",
    "                                 interpolation='nearest', vmin=0, vmax=1)        \n",
    "        self.ax.set_xticks(np.arange(12))\n",
    "        self.ax.set_xticks(np.arange(12) - 0.5, minor=True)\n",
    "        self.ax.set_yticks(np.arange(4))\n",
    "        self.ax.set_yticks(np.arange(4) - 0.5, minor=True)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.player = (3, 0)        \n",
    "        return self._position_to_id(self.player)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Possible actions\n",
    "        if action == 0 and self.player[0] > 0:\n",
    "            self.player = (self.player[0] - 1, self.player[1])\n",
    "        if action == 1 and self.player[0] < 3:\n",
    "            self.player = (self.player[0] + 1, self.player[1])\n",
    "        if action == 2 and self.player[1] < 11:\n",
    "            self.player = (self.player[0], self.player[1] + 1)\n",
    "        if action == 3 and self.player[1] > 0:\n",
    "            self.player = (self.player[0], self.player[1] - 1)\n",
    "            \n",
    "        # Rules\n",
    "        if all(self.grid[self.player] == self.terrain_color['cliff']):\n",
    "            reward = -100\n",
    "            done = True\n",
    "        elif all(self.grid[self.player] == self.terrain_color['objective']):\n",
    "            reward = 0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "            \n",
    "        return self._position_to_id(self.player), reward, done\n",
    "    \n",
    "    def _position_to_id(self, pos):\n",
    "        ''' Maps a position in x,y coordinates to a unique ID '''\n",
    "        return pos[0] * 12 + pos[1]\n",
    "    \n",
    "    def _id_to_position(self, idx):\n",
    "        return (idx // 12), (idx % 12)\n",
    "        \n",
    "    def render(self, q_values=None, action=None, max_q=False, colorize_q=False):\n",
    "        assert self.player is not None, 'You first need to call .reset()'  \n",
    "        \n",
    "        if colorize_q:\n",
    "            assert q_values is not None, 'q_values must not be None for using colorize_q'            \n",
    "            grid = self.terrain_color['normal'] * np.ones((4, 12, 3))\n",
    "            values = change_range(np.max(q_values, -1)).reshape(4, 12)\n",
    "            grid[:, :, 1] = values\n",
    "            self._add_objectives(grid)\n",
    "        else:            \n",
    "            grid = self.grid.copy()\n",
    "            \n",
    "        grid[self.player] = self.terrain_color['player']       \n",
    "        self.im.set_data(hsv_to_rgb(grid))\n",
    "               \n",
    "        if q_values is not None:\n",
    "            xs = np.repeat(np.arange(12), 4)\n",
    "            ys = np.tile(np.arange(4), 12)  \n",
    "            \n",
    "            for i, text in enumerate(self.q_texts):\n",
    "                if max_q:\n",
    "                    q = max(q_values[i])    \n",
    "                    txt = '{:.2f}'.format(q)\n",
    "                    text.set_text(txt)\n",
    "                else:                \n",
    "                    actions = ['U', 'D', 'R', 'L']\n",
    "                    txt = '\\n'.join(['{}: {:.2f}'.format(k, q) for k, q in zip(actions, q_values[i])])\n",
    "                    text.set_text(txt)\n",
    "                \n",
    "        if action is not None:\n",
    "            self.ax.set_title(action, color='r', weight='bold', fontsize=32)\n",
    "\n",
    "        plt.pause(0.001)\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_policy(q_values, state, epsilon = 0.2):\n",
    "    # This function gets a random number from a uniform distribution\n",
    "    # If the number is greater than epsilon, act greedily\n",
    "    # If the number is less than epsilon, choose randomly\n",
    "\n",
    "    # Parameters:\n",
    "    ## state: an integer describing the current state (row of Q table)\n",
    "    ## q_values: Q table of stored rewards\n",
    "    ## epsilon: the probability describing degree of encouraged exploration\n",
    "\n",
    "    # Returns:\n",
    "    ## An integer describing index of action chosen\n",
    "    if np.random.uniform() > epsilon:\n",
    "        # return the highest scoring action for a given state\n",
    "        return np.argmax(q_values[state,:])\n",
    "    else:\n",
    "        # Pick a random action of the available actions\n",
    "        return np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(q_values, state):\n",
    "    # This function selects the greediest action from a list\n",
    "    # of given actions (highest score). FOr any ties, the first\n",
    "    # greedy action is chosen\n",
    "\n",
    "    # Parameters:\n",
    "    ## state: an integer describing the current state (row of Q table)\n",
    "    ## q_values: Q table of stored rewards\n",
    "\n",
    "    # Returns:\n",
    "    ## An integer describing index of action chosen\n",
    "\n",
    "    return np.argmax(q_values[state,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_episode(env, q_values, state,  policy_func, alpha):\n",
    "    # This function runs one episode of the task, using a specified environment and policy\n",
    "    # Parameters:\n",
    "        ## env: The environment in which episode will be run in\n",
    "        ## q_values: The table of rewards stored through iterations\n",
    "        ## state: The starting state of the environment\n",
    "        ## action: Initial action to be taken by the agent\n",
    "        ## policy_func: handle of function describing learner policy\n",
    "        ## alpha: The user specified learning rate\n",
    "\n",
    "    # Returns:\n",
    "        ## Rewards: a list of rewards for actions chosen in the episode\n",
    "        ## Q_values: an updated state-action value table\n",
    "    total_reward = []\n",
    "    # Initialize done variable to a given value\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = egreedy_policy(q_values, state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # Evaluate the successor action a'\n",
    "        next_action = policy_func(q_values, next_state)\n",
    "        \n",
    "        # Evaluate the Temporal DIfference (TD) error\n",
    "        td_error = (reward + (gamma * q_values[next_state, next_action]) - q_values[state, action])\n",
    "\n",
    "        # Update the q_value table for the state-action pair \n",
    "        q_values[state, action] +=  (alpha * td_error)\n",
    "        \n",
    "        # Update the state and action to the next state and action\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "\n",
    "        # Append the reward to the list of total rewards for the episode so far\n",
    "        total_reward.append(reward)\n",
    "\n",
    "    # Convert the total rewards obtained into an array and return it along with updated state-action value table\n",
    "    total_reward = np.asarray(total_reward)\n",
    "    return (q_values,total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sarsa(env, q_values, state, policy_func, alpha):\n",
    "    # This function runs one episode of the task, using a specified environment and policy\n",
    "    # Parameters:\n",
    "        ## env: The environment in which episode will be run in\n",
    "        ## q_values: The table of rewards stored through iterations\n",
    "        ## state: The starting state of the environment\n",
    "        ## action: Initial action to be taken by the agent\n",
    "        ## policy_func: handle of function describing learner policy\n",
    "        ## alpha: The user specified learning rate\n",
    "\n",
    "\n",
    "    # Initialize done variable to a given value\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy_func(q_values, state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        env.render(q_values, colorize_q = True)\n",
    "\n",
    "        # print(f\"Now in state: {state}, with reward {reward} \")\n",
    "\n",
    "        # Evaluate the successor action a'\n",
    "        next_action = policy_func(q_values, next_state)\n",
    "        \n",
    "        # Update the state and action to the next state and action\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1012.45it/s]\n"
     ]
    }
   ],
   "source": [
    "### SARSA ALGORTIHM ###\n",
    "\n",
    "env = GridWorld()\n",
    "num_states = 48\n",
    "num_actions = 4\n",
    "sarsa_episode_rewards = []\n",
    "alpha = 0.5\n",
    "gamma = 1\n",
    "num_episodes = 500\n",
    "# Initialize the Q-values table\n",
    "sarsa_Q_values = np.zeros((num_states,num_actions))\n",
    "\n",
    "for i in tqdm(range(num_episodes)):\n",
    "    # Evaluate state action pair for initial state\n",
    "    state = env.reset()\n",
    "\n",
    "    # Run a Sarsa episode\n",
    "    sarsa_Q_values, r = sarsa_episode(env, sarsa_Q_values, state, egreedy_policy, alpha)\n",
    "    # Append the total reward to the episode reward\n",
    "    sarsa_episode_rewards.append(sum(r))\n",
    "# Close the training figure\n",
    "plt.close()\n",
    "# Visualize Sarsa results\n",
    "play_env = GridWorld()\n",
    "state = play_env.reset()\n",
    "action = egreedy_policy(sarsa_Q_values, state)\n",
    "play_env.render(sarsa_Q_values, colorize_q = True)\n",
    "play_sarsa(play_env, sarsa_Q_values, state, greedy_policy, alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(sarsa_episode_rewards)\n",
    "plt.show"
   ]
  }
 ]
}